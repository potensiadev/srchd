"""
Pipeline Orchestrator - íŒŒì´í”„ë¼ì¸ í†µí•© ê´€ë¦¬

main.pyì™€ tasks.pyì˜ íŒŒì´í”„ë¼ì¸ ë¡œì§ì„ í†µí•©í•˜ê³ 
PipelineContextë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import logging
import time
import asyncio
from typing import Optional, Dict, Any
from dataclasses import dataclass, field

from context import PipelineContext
from context.layers import PIIStore
from .feature_flags import get_feature_flags

logger = logging.getLogger(__name__)


def _get_metrics_collector():
    """Lazy import to avoid circular dependencies"""
    try:
        from services.metrics_service import get_metrics_collector
        return get_metrics_collector()
    except ImportError:
        logger.warning("MetricsCollector not available")
        return None


@dataclass
class OrchestratorResult:
    """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹¤í–‰ ê²°ê³¼"""
    success: bool
    candidate_id: Optional[str] = None
    confidence_score: float = 0.0
    field_confidence: Dict[str, float] = field(default_factory=dict)

    # ì²˜ë¦¬ ê²°ê³¼
    chunk_count: int = 0
    chunks_saved: int = 0
    pii_count: int = 0

    # ê²½ê³  ë° ì—ëŸ¬
    warnings: list = field(default_factory=list)
    error: Optional[str] = None
    error_code: Optional[str] = None

    # ë©”íƒ€ë°ì´í„°
    processing_time_ms: int = 0
    pipeline_id: Optional[str] = None
    is_update: bool = False
    parent_id: Optional[str] = None

    # Phase 1 ì¶”ê°€ í•„ë“œ
    document_kind: Optional[str] = None  # 'resume', 'non_resume', 'uncertain'
    doc_classification_confidence: float = 0.0
    coverage_score: float = 0.0
    gap_fill_count: int = 0

    # í’ˆì§ˆ í”Œë˜ê·¸ (BUG-003: fail-open ì¶”ì )
    quality_flags: Dict[str, bool] = field(default_factory=dict)
    # ê°€ëŠ¥í•œ í”Œë˜ê·¸:
    # - classification_failed: ë¬¸ì„œ ë¶„ë¥˜ ì‹¤íŒ¨
    # - classification_retried: ë¶„ë¥˜ ì¬ì‹œë„ ìˆ˜í–‰ë¨
    # - identity_check_failed: ì‹ ì› í™•ì¸ ì‹¤íŒ¨
    # - identity_check_retried: ì‹ ì› í™•ì¸ ì¬ì‹œë„ ìˆ˜í–‰ë¨
    # - coverage_calc_failed: ì»¤ë²„ë¦¬ì§€ ê³„ì‚° ì‹¤íŒ¨
    # - gap_fill_failed: ê°­ í•„ë§ ì‹¤íŒ¨
    # - low_confidence: confidence < 0.7
    # - quality_gate_warning: í’ˆì§ˆ ê²Œì´íŠ¸ ê²½ê³  (ë‚®ì€ ì»¤ë²„ë¦¬ì§€)

    # ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ìƒíƒœ
    completed_with_warnings: bool = False  # Trueë©´ í’ˆì§ˆ ì¡°ê±´ ë¯¸ë‹¬ (ì»¤ë²„ë¦¬ì§€ < ìµœì†Œê°’)
    quality_gate_passed: bool = True       # í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼ ì—¬ë¶€

    # ğŸŸ¡ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰
    total_input_tokens: int = 0
    total_output_tokens: int = 0

    # ë””ë²„ê·¸ ì •ë³´
    context_summary: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "success": self.success,
            "candidate_id": self.candidate_id,
            "confidence_score": self.confidence_score,
            "field_confidence": self.field_confidence,
            "chunk_count": self.chunk_count,
            "chunks_saved": self.chunks_saved,
            "pii_count": self.pii_count,
            "warnings": self.warnings,
            "error": self.error,
            "error_code": self.error_code,
            "processing_time_ms": self.processing_time_ms,
            "pipeline_id": self.pipeline_id,
            "is_update": self.is_update,
            "parent_id": self.parent_id,
            "document_kind": self.document_kind,
            "doc_classification_confidence": self.doc_classification_confidence,
            "coverage_score": self.coverage_score,
            "gap_fill_count": self.gap_fill_count,
            "quality_flags": self.quality_flags,
            "completed_with_warnings": self.completed_with_warnings,
            "quality_gate_passed": self.quality_gate_passed,
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
        }


class PipelineOrchestrator:
    """
    íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

    ì „ì²´ ì´ë ¥ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

    Stages:
    1. íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Storageì—ì„œ)
    2. íŒŒì¼ íŒŒì‹± (PDF/HWP/DOCX)
    2.5. ë¬¸ì„œ ë¶„ë¥˜ (ì´ë ¥ì„œ/ë¹„ì´ë ¥ì„œ) [Phase 1]
    3. PII ì¶”ì¶œ (ì •ê·œì‹ ì „ìš©)
    4. ì‹ ì› í™•ì¸ (Multi-Identity ì²´í¬)
    5. AI ë¶„ì„ (GPT + Gemini)
    6. ê²€ì¦ ë° í™˜ê° íƒì§€
    6.5. ì»¤ë²„ë¦¬ì§€ ê³„ì‚° [Phase 1]
    6.6. ê°­ í•„ë§ (ë¹ˆ í•„ë“œ ì¬ì¶”ì¶œ) [Phase 1]
    7. PII ë§ˆìŠ¤í‚¹ + ì•”í˜¸í™”
    8. ì„ë² ë”© ìƒì„±
    9. DB ì €ì¥
    """

    def __init__(self):
        self.feature_flags = get_feature_flags()
        self._init_agents()

    def _init_agents(self):
        """ì—ì´ì „íŠ¸ ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # Lazy import to avoid circular dependencies
        from agents.router_agent import RouterAgent
        from utils.hwp_parser import HWPParser
        from utils.pdf_parser import PDFParser
        from utils.docx_parser import DOCXParser
        from config import get_settings

        settings = get_settings()

        self.router_agent = RouterAgent()
        self.hwp_parser = HWPParser(hancom_api_key=settings.HANCOM_API_KEY or None)
        self.pdf_parser = PDFParser()
        self.docx_parser = DOCXParser()

        # Phase 1 ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (feature flagì— ë”°ë¼ ì§€ì—° ì´ˆê¸°í™”)
        self._document_classifier = None
        self._coverage_calculator = None
        self._gap_filler_agent = None

        # P1 ì •í™•ë„ í–¥ìƒ: Field-Based Analyst (feature flagì— ë”°ë¼ ì§€ì—° ì´ˆê¸°í™”)
        self._field_based_analyst = None

    def _get_document_classifier(self):
        """DocumentClassifier ì§€ì—° ì´ˆê¸°í™”"""
        if self._document_classifier is None and self.feature_flags.use_document_classifier:
            from agents.document_classifier import DocumentClassifier
            self._document_classifier = DocumentClassifier()
        return self._document_classifier

    def _get_coverage_calculator(self):
        """CoverageCalculator ì§€ì—° ì´ˆê¸°í™”"""
        if self._coverage_calculator is None and self.feature_flags.use_coverage_calculator:
            from agents.coverage_calculator import CoverageCalculator
            self._coverage_calculator = CoverageCalculator()
        return self._coverage_calculator

    def _get_gap_filler_agent(self):
        """GapFillerAgent ì§€ì—° ì´ˆê¸°í™”"""
        if self._gap_filler_agent is None and self.feature_flags.use_gap_filler:
            from agents.gap_filler_agent import GapFillerAgent
            from services.llm_manager import get_llm_manager
            llm_manager = get_llm_manager()
            self._gap_filler_agent = GapFillerAgent(
                llm_manager=llm_manager,
                max_retries=self.feature_flags.gap_filler_max_retries,
                timeout_seconds=self.feature_flags.gap_filler_timeout,
                coverage_threshold=self.feature_flags.coverage_threshold,
            )
        return self._gap_filler_agent

    def _get_field_based_analyst(self):
        """FieldBasedAnalyst ì§€ì—° ì´ˆê¸°í™”"""
        if self._field_based_analyst is None and self.feature_flags.use_field_based_analyst:
            from agents.field_based_analyst import get_field_based_analyst
            self._field_based_analyst = get_field_based_analyst()
        return self._field_based_analyst

    async def run(
        self,
        file_bytes: bytes,
        filename: str,
        user_id: str,
        job_id: str,
        mode: str = "phase_1",
        candidate_id: Optional[str] = None,
        is_retry: bool = False,
    ) -> OrchestratorResult:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            file_bytes: íŒŒì¼ ë°”ì´íŠ¸
            filename: íŒŒì¼ëª…
            user_id: ì‚¬ìš©ì ID
            job_id: ì‘ì—… ID
            mode: ë¶„ì„ ëª¨ë“œ (phase_1 or phase_2)
            candidate_id: ê¸°ì¡´ í›„ë³´ì ID (ì—…ë°ì´íŠ¸ìš©)
            is_retry: ì¬ì‹œë„ ì—¬ë¶€

        Returns:
            OrchestratorResult with processing results
        """
        start_time = time.time()

        # PipelineContext ìƒì„±
        ctx = PipelineContext()
        ctx.metadata.candidate_id = candidate_id
        ctx.metadata.job_id = job_id
        ctx.metadata.user_id = user_id
        ctx.metadata.config["mode"] = mode

        logger.info(f"[Orchestrator] Starting pipeline: {ctx.metadata.pipeline_id}")

        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
        metrics_collector = _get_metrics_collector()
        if metrics_collector:
            metrics_collector.start_pipeline(
                pipeline_id=ctx.metadata.pipeline_id,
                job_id=job_id,
                user_id=user_id,
                pipeline_type="new",
                is_retry=is_retry,
            )

        try:
            # Stage 1: ì›ë³¸ ì…ë ¥ ì„¤ì •
            ctx.set_raw_input(file_bytes, filename, source="upload")

            # Stage 2: íŒŒì‹±
            parse_result = await self._stage_parsing(ctx)
            if not parse_result["success"]:
                return self._create_error_result(
                    ctx, parse_result["error"], "PARSE_FAILED", start_time
                )

            # Stage 2.5: ë¬¸ì„œ ë¶„ë¥˜ (Phase 1)
            classification_result = await self._stage_document_classification(ctx)
            if classification_result.get("should_reject"):
                return self._create_error_result(
                    ctx, classification_result["error"], "NOT_RESUME", start_time
                )

            # Stage 3: PII ì¶”ì¶œ (ì •ê·œì‹ ì „ìš©)
            await self._stage_pii_extraction(ctx)

            # Stage 4: ì‹ ì› í™•ì¸
            identity_result = await self._stage_identity_check(ctx)
            if identity_result.get("should_reject"):
                return self._create_error_result(
                    ctx, identity_result["error"], "MULTI_IDENTITY", start_time
                )

            # Stage 5: AI ë¶„ì„
            analysis_result = await self._stage_analysis(ctx, mode)
            if not analysis_result["success"]:
                return self._create_error_result(
                    ctx, analysis_result["error"], "ANALYSIS_FAILED", start_time
                )

            # Stage 6: ê²€ì¦ ë° í™˜ê° íƒì§€
            await self._stage_validation(ctx)

            # Stage 6.5: ì»¤ë²„ë¦¬ì§€ ê³„ì‚° (Phase 1)
            coverage_result = await self._stage_coverage_calculation(ctx)

            # Stage 6.6: ê°­ í•„ë§ (Phase 1)
            gap_fill_result = await self._stage_gap_filling(ctx, coverage_result)

            # ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬
            quality_gate_result = self._check_quality_gate(coverage_result, ctx)

            # Stage 7: PII ë§ˆìŠ¤í‚¹ + ì•”í˜¸í™”
            privacy_result = await self._stage_privacy(ctx)

            # Stage 8: ì„ë² ë”© ìƒì„±
            embedding_result = await self._stage_embedding(ctx)

            # Stage 9: DB ì €ì¥
            save_result = await self._stage_save(ctx, user_id, job_id, mode, candidate_id)
            if not save_result["success"]:
                return self._create_error_result(
                    ctx, save_result["error"], "DB_SAVE_FAILED", start_time
                )

            # ì™„ë£Œ
            final_result = ctx.finalize()
            processing_time = int((time.time() - start_time) * 1000)

            # ë©”íŠ¸ë¦­ ì™„ë£Œ ê¸°ë¡
            if metrics_collector:
                metrics_collector.complete_pipeline(
                    pipeline_id=ctx.metadata.pipeline_id,
                    success=True,
                    text_length=len(ctx.parsed_data.raw_text) if ctx.parsed_data.raw_text else 0,
                    chunk_count=embedding_result.get("chunk_count", 0),
                    pii_count=privacy_result.get("pii_count", 0),
                    confidence_score=final_result["confidence"],
                )

            logger.info(
                f"[Orchestrator] Pipeline completed: {ctx.metadata.pipeline_id}, "
                f"candidate={save_result['candidate_id']}, time={processing_time}ms"
            )

            # í’ˆì§ˆ í”Œë˜ê·¸ ìˆ˜ì§‘ (BUG-003: fail-open ì¶”ì )
            quality_flags = {}
            if classification_result.get("quality_flag"):
                flag = classification_result["quality_flag"]
                quality_flags[flag] = True
            if identity_result.get("quality_flag"):
                flag = identity_result["quality_flag"]
                quality_flags[flag] = True
            if coverage_result.get("quality_flag"):
                quality_flags["coverage_calc_failed"] = True
            if gap_fill_result.get("quality_flag"):
                quality_flags["gap_fill_failed"] = True
            if final_result["confidence"] < 0.7:
                quality_flags["low_confidence"] = True
            # ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ê²½ê³ 
            if quality_gate_result.get("quality_flag"):
                quality_flags["quality_gate_warning"] = True

            # í’ˆì§ˆ ì €í•˜ ì‹œ ë¡œê¹…
            if quality_flags:
                logger.warning(
                    f"[Orchestrator] Quality flags detected: {list(quality_flags.keys())}"
                )

            # ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ê²°ê³¼ ë°˜ì˜
            completed_with_warnings = not quality_gate_result.get("passed", True)
            quality_gate_passed = quality_gate_result.get("passed", True)

            return OrchestratorResult(
                success=True,
                candidate_id=save_result["candidate_id"],
                confidence_score=final_result["confidence"],
                field_confidence=dict(ctx.current_data.confidence_scores),
                chunk_count=embedding_result.get("chunk_count", 0),
                chunks_saved=save_result.get("chunks_saved", 0),
                pii_count=privacy_result.get("pii_count", 0),
                warnings=[w["message"] for w in final_result.get("warnings", [])],
                processing_time_ms=processing_time,
                pipeline_id=ctx.metadata.pipeline_id,
                is_update=save_result.get("is_update", False),
                parent_id=save_result.get("parent_id"),
                document_kind=classification_result.get("document_kind"),
                doc_classification_confidence=classification_result.get("confidence", 0.0),
                coverage_score=coverage_result.get("coverage_score", 0.0),
                gap_fill_count=gap_fill_result.get("filled_count", 0),
                quality_flags=quality_flags,
                completed_with_warnings=completed_with_warnings,
                quality_gate_passed=quality_gate_passed,
                # ğŸŸ¡ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰
                total_input_tokens=analysis_result.get("total_input_tokens", 0),
                total_output_tokens=analysis_result.get("total_output_tokens", 0),
                context_summary=ctx.to_dict() if self.feature_flags.debug_pipeline else None,
            )

        except Exception as e:
            logger.error(f"[Orchestrator] Pipeline error: {e}", exc_info=True)
            # ë©”íŠ¸ë¦­ ì—ëŸ¬ ê¸°ë¡
            if metrics_collector:
                metrics_collector.complete_pipeline(
                    pipeline_id=ctx.metadata.pipeline_id,
                    success=False,
                    error_code="INTERNAL_ERROR",
                    error_message=str(e)[:200],
                )
            return self._create_error_result(ctx, str(e), "INTERNAL_ERROR", start_time)

    async def _stage_parsing(self, ctx: PipelineContext) -> Dict[str, Any]:
        """Stage 2: íŒŒì¼ íŒŒì‹±"""
        from agents.router_agent import FileType
        from utils.hwp_parser import ParseMethod

        stage_start = time.time()
        ctx.start_stage("parsing", "router_agent")

        try:
            file_bytes = ctx.raw_input.file_bytes
            filename = ctx.raw_input.filename

            # Router Agentë¡œ íŒŒì¼ ë¶„ì„
            router_result = self.router_agent.analyze(file_bytes, filename)

            if router_result.is_rejected:
                ctx.fail_stage("parsing", router_result.reject_reason, "FILE_REJECTED")
                return {"success": False, "error": router_result.reject_reason}

            # íŒŒì„œ ì„ íƒ ë° íŒŒì‹±
            text = ""
            parse_method = "unknown"
            page_count = 0

            if router_result.file_type in [FileType.HWP, FileType.HWPX]:
                result = self.hwp_parser.parse(file_bytes, filename)
                text = result.text
                parse_method = result.method.value
                page_count = result.page_count

                if result.method == ParseMethod.FAILED:
                    ctx.fail_stage("parsing", result.error_message, "HWP_PARSE_FAILED")
                    return {"success": False, "error": result.error_message}

            elif router_result.file_type == FileType.PDF:
                result = self.pdf_parser.parse(file_bytes)
                text = result.text
                parse_method = result.method
                page_count = result.page_count

                if not result.success:
                    ctx.fail_stage("parsing", result.error_message, "PDF_PARSE_FAILED")
                    return {"success": False, "error": result.error_message}

            elif router_result.file_type in [FileType.DOC, FileType.DOCX]:
                result = self.docx_parser.parse(file_bytes, filename)
                text = result.text
                parse_method = result.method
                page_count = result.page_count

                if not result.success:
                    ctx.fail_stage("parsing", result.error_message, "DOCX_PARSE_FAILED")
                    return {"success": False, "error": result.error_message}
            else:
                error = f"Unsupported file type: {router_result.file_type}"
                ctx.fail_stage("parsing", error, "UNSUPPORTED_TYPE")
                return {"success": False, "error": error}

            # í…ìŠ¤íŠ¸ ì„¤ì •
            ctx.set_parsed_text(
                text,
                parsing_method=parse_method,
                parsing_confidence=0.9 if page_count > 0 else 0.7
            )

            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì²´í¬
            from config import get_settings
            settings = get_settings()

            if len(text.strip()) < settings.MIN_TEXT_LENGTH:
                ctx.warning_collector.add_parsing_issue(
                    f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(text.strip())}ì)"
                )

            ctx.complete_stage("parsing", {
                "text_length": len(text),
                "page_count": page_count,
                "parse_method": parse_method,
                "file_type": router_result.file_type.value,
            })

            # ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ê¸°ë¡
            stage_duration = int((time.time() - stage_start) * 1000)
            metrics_collector = _get_metrics_collector()
            if metrics_collector:
                metrics_collector.record_stage(ctx.metadata.pipeline_id, "parsing", stage_duration)

            logger.info(f"[Orchestrator] Parsing complete: {len(text)} chars, {page_count} pages")
            return {"success": True, "text": text}

        except Exception as e:
            ctx.fail_stage("parsing", str(e))
            return {"success": False, "error": str(e)}

    async def _stage_document_classification(self, ctx: PipelineContext) -> Dict[str, Any]:
        """
        Stage 2.5: ë¬¸ì„œ ë¶„ë¥˜ (Phase 1)

        ì´ë ¥ì„œ vs ë¹„ì´ë ¥ì„œ ë¶„ë¥˜ í›„ ë¹„ì´ë ¥ì„œëŠ” ê±°ë¶€í•©ë‹ˆë‹¤.
        ğŸŸ¡ ì¡°ê±´ë¶€ fail-closed: ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ í›„ í™•ì •
        """
        if not self.feature_flags.use_document_classifier:
            logger.debug("[Orchestrator] Document classification disabled")
            return {"success": True, "document_kind": "resume", "confidence": 1.0}

        ctx.start_stage("document_classification", "document_classifier")

        max_attempts = 1 + (
            self.feature_flags.max_classification_retries
            if self.feature_flags.enable_classification_retry else 0
        )
        last_error = None
        retried = False

        for attempt in range(max_attempts):
            try:
                classifier = self._get_document_classifier()
                if not classifier:
                    ctx.complete_stage("document_classification", {"skipped": True})
                    return {"success": True, "document_kind": "resume", "confidence": 1.0}

                text = ctx.parsed_data.raw_text
                filename = ctx.raw_input.filename

                # ì¬ì‹œë„ ì‹œ LLM fallback ê°•ì œ (confidence_thresholdë¥¼ 0ìœ¼ë¡œ ì„¤ì •)
                confidence_threshold = (
                    0.0 if attempt > 0
                    else self.feature_flags.document_classifier_confidence_threshold
                )

                result = await classifier.classify(
                    text, filename,
                    confidence_threshold=confidence_threshold
                )

                # UNCERTAIN ê²°ê³¼ë©´ì„œ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë©´ ì¬ì‹œë„
                if result.document_kind.value == "uncertain" and attempt < max_attempts - 1:
                    logger.info(
                        f"[Orchestrator] Classification uncertain (attempt {attempt + 1}), retrying with LLM..."
                    )
                    retried = True
                    continue

                ctx.complete_stage("document_classification", {
                    "document_kind": result.document_kind.value,
                    "confidence": result.confidence,
                    "non_resume_type": result.non_resume_type.value if result.non_resume_type else None,
                    "signals_found": len(result.signals_found),
                    "used_llm": result.used_llm,
                    "attempts": attempt + 1,
                    "retried": retried,
                })

                # ì´ë ¥ì„œê°€ ì•„ë‹Œ ê²½ìš° ê±°ë¶€
                if result.should_reject:
                    error_msg = f"ì´ë ¥ì„œê°€ ì•„ë‹™ë‹ˆë‹¤: {result.rejection_reason}"
                    ctx.warning_collector.add(
                        "NOT_RESUME",
                        error_msg,
                        severity="error"
                    )
                    logger.info(
                        f"[Orchestrator] Document rejected: {result.document_kind.value}, "
                        f"reason={result.rejection_reason}"
                    )
                    return {
                        "success": False,
                        "should_reject": True,
                        "error": error_msg,
                        "document_kind": result.document_kind.value,
                        "confidence": result.confidence,
                    }

                logger.info(
                    f"[Orchestrator] Document classified: {result.document_kind.value}, "
                    f"confidence={result.confidence:.2f}, attempts={attempt + 1}"
                )
                return_result = {
                    "success": True,
                    "document_kind": result.document_kind.value,
                    "confidence": result.confidence,
                }
                if retried:
                    return_result["quality_flag"] = "classification_retried"
                return return_result

            except Exception as e:
                last_error = e
                logger.warning(
                    f"[Orchestrator] Document classification error (attempt {attempt + 1}): {e}"
                )
                if attempt < max_attempts - 1:
                    retried = True
                    await asyncio.sleep(0.5)  # ì¬ì‹œë„ ì „ ì§§ì€ ëŒ€ê¸°
                    continue

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        logger.warning(f"[Orchestrator] Document classification failed after {max_attempts} attempts")
        ctx.complete_stage("document_classification", {
            "error": str(last_error),
            "quality_degraded": True,
            "attempts": max_attempts,
        })
        ctx.warning_collector.add(
            "CLASSIFICATION_FAILED",
            f"ë¬¸ì„œ ë¶„ë¥˜ ì‹¤íŒ¨ë¡œ ì´ë ¥ì„œë¡œ ê°€ì •í•˜ì—¬ ì²˜ë¦¬ë¨: {str(last_error)[:100]}",
            severity="warning",
            user_visible=True
        )
        return {
            "success": True,
            "document_kind": "uncertain",
            "confidence": 0.0,
            "quality_flag": "classification_failed"
        }

    async def _stage_pii_extraction(self, ctx: PipelineContext) -> Dict[str, Any]:
        """Stage 3: PII ì¶”ì¶œ (ì •ê·œì‹ ì „ìš©)"""
        ctx.start_stage("pii_extraction", "pii_extractor")

        try:
            # PipelineContextì˜ PII ì¶”ì¶œ ë©”ì„œë“œ ì‚¬ìš©
            ctx.extract_pii()

            # PII ê¸°ë°˜ ì œì•ˆ ì¶”ê°€
            if ctx.pii_store.name:
                ctx.propose(
                    "pii_extractor", "name",
                    ctx.pii_store.name,
                    ctx.pii_store.name_confidence,
                    f"ì •ê·œì‹ ì¶”ì¶œ ({ctx.pii_store.name_source})"
                )

            if ctx.pii_store.phone:
                ctx.propose(
                    "pii_extractor", "phone",
                    ctx.pii_store.phone,
                    ctx.pii_store.phone_confidence,
                    "ì •ê·œì‹ ì¶”ì¶œ"
                )

            if ctx.pii_store.email:
                ctx.propose(
                    "pii_extractor", "email",
                    ctx.pii_store.email,
                    ctx.pii_store.email_confidence,
                    "ì •ê·œì‹ ì¶”ì¶œ"
                )

            ctx.complete_stage("pii_extraction", {
                "name": bool(ctx.pii_store.name),
                "phone": bool(ctx.pii_store.phone),
                "email": bool(ctx.pii_store.email),
            })

            return {"success": True}

        except Exception as e:
            ctx.fail_stage("pii_extraction", str(e))
            return {"success": False, "error": str(e)}

    async def _stage_identity_check(self, ctx: PipelineContext) -> Dict[str, Any]:
        """
        Stage 4: ì‹ ì› í™•ì¸ (Multi-Identity ì²´í¬)
        ğŸŸ¡ ì¡°ê±´ë¶€ fail-closed: ì‹ ì› í™•ì¸ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ í›„ í™•ì •
        """
        ctx.start_stage("identity_check", "identity_checker")

        max_attempts = 1 + (
            self.feature_flags.max_identity_check_retries
            if self.feature_flags.enable_identity_check_retry else 0
        )
        last_error = None
        retried = False

        for attempt in range(max_attempts):
            try:
                from agents.identity_checker import get_identity_checker

                identity_checker = get_identity_checker()
                text = ctx.parsed_data.raw_text

                result = await identity_checker.check(text)

                if result.should_reject:
                    error = f"ë‹¤ì¤‘ ì‹ ì› ê°ì§€: {result.person_count}ëª…ì˜ ì •ë³´ ({result.reason})"
                    ctx.fail_stage("identity_check", error, "MULTI_IDENTITY")
                    ctx.warning_collector.add(
                        "MULTI_IDENTITY",
                        error,
                        severity="error"
                    )
                    return {"success": False, "should_reject": True, "error": error}

                ctx.complete_stage("identity_check", {
                    "person_count": result.person_count,
                    "result": result.result.value,
                    "attempts": attempt + 1,
                    "retried": retried,
                })

                return_result = {"success": True, "should_reject": False}
                if retried:
                    return_result["quality_flag"] = "identity_check_retried"
                return return_result

            except Exception as e:
                last_error = e
                logger.warning(
                    f"[Orchestrator] Identity check error (attempt {attempt + 1}): {e}"
                )
                if attempt < max_attempts - 1:
                    retried = True
                    await asyncio.sleep(0.5)  # ì¬ì‹œë„ ì „ ì§§ì€ ëŒ€ê¸°
                    continue

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        logger.warning(f"[Orchestrator] Identity check failed after {max_attempts} attempts")
        ctx.warning_collector.add(
            "IDENTITY_CHECK_FAILED",
            f"ì‹ ì› í™•ì¸ ì‹¤íŒ¨ë¡œ ê²€ì¦ ìƒëµë¨: {str(last_error)[:100]}",
            severity="warning",
            user_visible=True
        )
        ctx.complete_stage("identity_check", {
            "skipped": True,
            "error": str(last_error),
            "quality_degraded": True,
            "attempts": max_attempts,
        })
        return {
            "success": True,
            "should_reject": False,
            "quality_flag": "identity_check_failed"
        }

    async def _stage_analysis(self, ctx: PipelineContext, mode: str) -> Dict[str, Any]:
        """Stage 5: AI ë¶„ì„"""
        stage_start = time.time()

        # Feature Flag: FieldBasedAnalyst ì‚¬ìš© ì—¬ë¶€
        if self.feature_flags.use_field_based_analyst:
            return await self._stage_field_based_analysis(ctx, mode, stage_start)

        # ê¸°ì¡´ AnalystAgent ì‚¬ìš©
        ctx.start_stage("analysis", "analyst_agent")

        try:
            from agents.analyst_agent import get_analyst_agent
            from config import AnalysisMode

            # ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸ ì‚¬ìš© (PII ë³´í˜¸)
            text = ctx.get_text_for_llm()
            filename = ctx.raw_input.filename

            analysis_mode = AnalysisMode.PHASE_2 if mode == "phase_2" else AnalysisMode.PHASE_1

            analyst = get_analyst_agent()
            result = await analyst.analyze(
                resume_text=text,
                mode=analysis_mode,
                filename=filename
            )

            if not result.success or not result.data:
                error = result.error or "ë¶„ì„ ì‹¤íŒ¨"
                ctx.fail_stage("analysis", error, "ANALYSIS_FAILED")
                return {"success": False, "error": error}

            # T3-2: LLM ì‚¬ìš©ëŸ‰ ê¸°ë¡ (ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰)
            total_tokens = result.total_input_tokens + result.total_output_tokens
            ctx.record_llm_call("analysis", total_tokens)

            # ë¶„ì„ ê²°ê³¼ë¥¼ ì œì•ˆìœ¼ë¡œ ë³€í™˜
            self._process_analysis_result(ctx, result)

            ctx.complete_stage("analysis", {
                "confidence_score": result.confidence_score,
                "warning_count": len(result.warnings),
                "mode": analysis_mode.value,
                # ğŸŸ¡ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ê°€
                "total_input_tokens": result.total_input_tokens,
                "total_output_tokens": result.total_output_tokens,
                "providers_used": result.providers_used,
            })

            # ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ê¸°ë¡
            stage_duration = int((time.time() - stage_start) * 1000)
            metrics_collector = _get_metrics_collector()
            if metrics_collector:
                metrics_collector.record_stage(ctx.metadata.pipeline_id, "analysis", stage_duration)

                # T3-2: per_provider_usage í™œìš©í•˜ì—¬ ì •í™•í•œ ë©”íŠ¸ë¦­ ê¸°ë¡
                for provider, usage in result.per_provider_usage.items():
                    metrics_collector.record_llm_call(
                        ctx.metadata.pipeline_id,
                        provider,
                        usage.get("model", provider),  # ì‹¤ì œ ëª¨ë¸ëª… ì‚¬ìš©
                        tokens_input=usage.get("input", 0),
                        tokens_output=usage.get("output", 0),
                    )

            logger.info(
                f"[Orchestrator] Analysis complete: confidence={result.confidence_score:.2f}, "
                f"tokens_in={result.total_input_tokens}, tokens_out={result.total_output_tokens}"
            )
            return {
                "success": True,
                "result": result,
                "total_input_tokens": result.total_input_tokens,
                "total_output_tokens": result.total_output_tokens,
            }

        except Exception as e:
            ctx.fail_stage("analysis", str(e))
            return {"success": False, "error": str(e)}

    async def _stage_field_based_analysis(
        self,
        ctx: PipelineContext,
        mode: str,
        stage_start: float
    ) -> Dict[str, Any]:
        """
        Stage 5 (Alternative): Field-Based Analystë¥¼ ì‚¬ìš©í•œ ë¶„ì„

        6ê°œì˜ ì „ë¬¸ Extractorë¡œ ë³‘ë ¬ ì¶”ì¶œ í›„ í•©ì˜ ë„ì¶œ
        """
        ctx.start_stage("analysis", "field_based_analyst")

        try:
            from services.llm_manager import LLMProvider

            field_analyst = self._get_field_based_analyst()
            if not field_analyst:
                logger.warning("[Orchestrator] FieldBasedAnalyst not available, falling back to AnalystAgent")
                ctx.complete_stage("analysis", {"fallback": True})
                # Fallback to standard analysis
                return await self._stage_analysis_fallback(ctx, mode, stage_start)

            # ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸ ì‚¬ìš© (PII ë³´í˜¸)
            text = ctx.get_text_for_llm()
            filename = ctx.raw_input.filename

            # Provider ì„¤ì •
            providers = [LLMProvider.OPENAI, LLMProvider.GEMINI]
            if self.feature_flags.field_analyst_providers:
                providers = [
                    LLMProvider(p) for p in self.feature_flags.field_analyst_providers
                    if p in [e.value for e in LLMProvider]
                ]

            # êµì°¨ê²€ì¦ í™œì„±í™” ì—¬ë¶€
            enable_cross_validation = not self.feature_flags.use_conditional_cross_validation

            result = await field_analyst.analyze(
                text=text,
                filename=filename,
                enable_cross_validation=enable_cross_validation,
                providers=providers
            )

            if not result.success:
                error = result.error or "Field-Based ë¶„ì„ ì‹¤íŒ¨"
                ctx.fail_stage("analysis", error, "FIELD_ANALYSIS_FAILED")
                return {"success": False, "error": error}

            # T3-2: LLM ì‚¬ìš©ëŸ‰ ê¸°ë¡
            total_tokens = result.total_input_tokens + result.total_output_tokens
            ctx.record_llm_call("field_analysis", total_tokens)

            # ë¶„ì„ ê²°ê³¼ë¥¼ ì œì•ˆìœ¼ë¡œ ë³€í™˜
            self._process_field_based_result(ctx, result)

            ctx.complete_stage("analysis", {
                "confidence_score": result.overall_confidence,
                "warning_count": len(result.warnings),
                "mode": "field_based",
                "total_input_tokens": result.total_input_tokens,
                "total_output_tokens": result.total_output_tokens,
                "providers_used": result.providers_used,
                "extractors_used": result.extractors_used,
                "cross_validation": result.cross_validation_performed,
                "quality_gate_passed": result.quality_gate_passed,
            })

            # ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ê¸°ë¡
            stage_duration = int((time.time() - stage_start) * 1000)
            metrics_collector = _get_metrics_collector()
            if metrics_collector:
                metrics_collector.record_stage(ctx.metadata.pipeline_id, "analysis", stage_duration)

                for provider in result.providers_used:
                    tokens_per_provider = (
                        result.total_input_tokens // max(1, len(result.providers_used)),
                        result.total_output_tokens // max(1, len(result.providers_used))
                    )
                    metrics_collector.record_llm_call(
                        ctx.metadata.pipeline_id,
                        provider,
                        "gpt-4o" if provider == "openai" else provider,
                        tokens_input=tokens_per_provider[0],
                        tokens_output=tokens_per_provider[1],
                    )

            logger.info(
                f"[Orchestrator] Field-Based Analysis complete: "
                f"confidence={result.overall_confidence:.2f}, "
                f"extractors={len(result.extractors_used)}, "
                f"cross_val={result.cross_validation_performed}, "
                f"tokens_in={result.total_input_tokens}, tokens_out={result.total_output_tokens}"
            )

            return {
                "success": True,
                "result": result,
                "total_input_tokens": result.total_input_tokens,
                "total_output_tokens": result.total_output_tokens,
            }

        except Exception as e:
            logger.error(f"[Orchestrator] Field-Based Analysis error: {e}", exc_info=True)
            ctx.fail_stage("analysis", str(e))
            return {"success": False, "error": str(e)}

    async def _stage_analysis_fallback(
        self,
        ctx: PipelineContext,
        mode: str,
        stage_start: float
    ) -> Dict[str, Any]:
        """FieldBasedAnalyst ì‚¬ìš© ë¶ˆê°€ ì‹œ ê¸°ì¡´ AnalystAgentë¡œ Fallback"""
        from agents.analyst_agent import get_analyst_agent
        from config import AnalysisMode

        text = ctx.get_text_for_llm()
        filename = ctx.raw_input.filename
        analysis_mode = AnalysisMode.PHASE_2 if mode == "phase_2" else AnalysisMode.PHASE_1

        analyst = get_analyst_agent()
        result = await analyst.analyze(
            resume_text=text,
            mode=analysis_mode,
            filename=filename
        )

        if not result.success or not result.data:
            error = result.error or "ë¶„ì„ ì‹¤íŒ¨"
            return {"success": False, "error": error}

        self._process_analysis_result(ctx, result)

        return {
            "success": True,
            "result": result,
            "total_input_tokens": result.total_input_tokens,
            "total_output_tokens": result.total_output_tokens,
        }

    def _process_field_based_result(self, ctx: PipelineContext, result):
        """
        FieldBasedAnalyst ê²°ê³¼ë¥¼ PipelineContext ì œì•ˆìœ¼ë¡œ ë³€í™˜
        """
        data = result.data
        field_confidence = result.confidence_map

        # ëª¨ë“  í•„ë“œ ì œì•ˆ
        for field_name, value in data.items():
            if value is None:
                continue

            confidence = field_confidence.get(field_name, 0.7)

            # ì¦ê±° ì¶”ê°€
            if self.feature_flags.use_evidence_tracking:
                ctx.add_evidence(
                    field_name=field_name,
                    value=value,
                    llm_provider="field_based_analyst",
                    confidence=confidence,
                    reasoning="Field-Based Analyst ì¶”ì¶œ"
                )

            # ì œì•ˆ ì¶”ê°€
            if isinstance(value, list):
                ctx.propose(
                    "field_based_analyst", field_name,
                    value,
                    confidence,
                    f"Field-Based ì¶”ì¶œ ({len(value)}ê°œ)"
                )
            else:
                ctx.propose(
                    "field_based_analyst", field_name,
                    value,
                    confidence,
                    "Field-Based ì¶”ì¶œ"
                )

        # ê²½ê³  ë³€í™˜
        for warning in result.warnings:
            ctx.warning_collector.add(
                "FIELD_ANALYST_WARNING",
                warning if isinstance(warning, str) else str(warning),
                severity="info"
            )

        # í’ˆì§ˆ ê²½ê³  ì¶”ê°€
        for quality_warning in result.quality_warnings:
            ctx.warning_collector.add(
                "QUALITY_WARNING",
                quality_warning,
                severity="warning",
                user_visible=True
            )

    def _process_analysis_result(self, ctx: PipelineContext, result):
        """ë¶„ì„ ê²°ê³¼ë¥¼ PipelineContext ì œì•ˆìœ¼ë¡œ ë³€í™˜"""
        data = result.data
        field_confidence = result.field_confidence

        # ì£¼ìš” í•„ë“œ ì œì•ˆ (ëª¨ë“  ìŠ¤ì¹¼ë¼ í•„ë“œ í¬í•¨)
        fields_to_propose = [
            "exp_years", "current_company", "current_position",
            "summary", "last_company", "last_position",
            # ì¶”ê°€ í•„ë“œë“¤
            "birth_year", "gender", "address", "location_city",
            "education_level", "education_school", "education_major",
            "match_reason",
            "portfolio_url", "github_url", "linkedin_url",
        ]

        for field_name in fields_to_propose:
            if data.get(field_name) is not None:
                confidence = field_confidence.get(field_name, 0.7)

                # ì¦ê±° ì¶”ê°€
                if self.feature_flags.use_evidence_tracking:
                    ctx.add_evidence(
                        field_name=field_name,
                        value=data[field_name],
                        llm_provider="analyst_agent",
                        confidence=confidence,
                        reasoning=f"LLM ë¶„ì„ ê²°ê³¼"
                    )

                # ì œì•ˆ ì¶”ê°€
                ctx.propose(
                    "analyst_agent", field_name,
                    data[field_name],
                    confidence,
                    "LLM ë¶„ì„ ê²°ê³¼"
                )

        # ë°°ì—´ í•„ë“œ
        array_fields = ["careers", "educations", "skills", "certifications", "projects", "strengths"]
        for field_name in array_fields:
            if data.get(field_name):
                confidence = field_confidence.get(field_name, 0.7)
                ctx.propose(
                    "analyst_agent", field_name,
                    data[field_name],
                    confidence,
                    f"LLM ë¶„ì„ ê²°ê³¼ ({len(data[field_name])}ê°œ)"
                )

        # ê²½ê³  ë³€í™˜
        for warning in result.warnings:
            ctx.warning_collector.add(
                warning.code if hasattr(warning, 'code') else "LLM_WARNING",
                warning.message if hasattr(warning, 'message') else str(warning),
                severity="info"
            )

    async def _stage_validation(self, ctx: PipelineContext) -> Dict[str, Any]:
        """Stage 6: ê²€ì¦ ë° í™˜ê° íƒì§€"""
        ctx.start_stage("validation", "validation_agent")

        try:
            # í˜„ì¬ ê²°ì •ëœ ë°ì´í„° ìˆ˜ì§‘
            decisions = ctx.decision_manager.decide_all()
            analyzed_data = {
                name: d.final_value for name, d in decisions.items()
                if d.final_value is not None
            }

            # LLM ê²€ì¦ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
            if self.feature_flags.use_llm_validation:
                # ìƒˆë¡œìš´ ValidationWrapper ì‚¬ìš© (LLM ê²€ì¦ í¬í•¨)
                from .validation_wrapper import get_validation_wrapper

                validation_wrapper = get_validation_wrapper()
                result = await validation_wrapper.validate(ctx, analyzed_data)

                if result.success:
                    # ê²€ì¦ëœ ë°ì´í„°ë¡œ ì¶”ê°€ ì œì•ˆ
                    for field_name, value in result.validated_data.items():
                        if value != analyzed_data.get(field_name):
                            boost = result.confidence_adjustments.get(field_name, 0)
                            ctx.propose(
                                "validation_wrapper", field_name,
                                value,
                                0.8 + boost,
                                "LLM+Regex ê²€ì¦ ê²°ê³¼"
                            )

                    # ë³´ì • ì‚¬í•­ ë¡œê¹…
                    for correction in result.regex_corrections:
                        logger.info(
                            f"[Orchestrator] Regex correction: {correction['field']}: "
                            f"{correction['original']} â†’ {correction['corrected']}"
                        )
                    for correction in result.llm_corrections:
                        logger.info(
                            f"[Orchestrator] LLM correction: {correction['field']}: "
                            f"{correction['original']} â†’ {correction['corrected']} "
                            f"(by {correction.get('llm_provider', 'unknown')})"
                        )

                ctx.complete_stage("validation", {
                    "regex_corrections": len(result.regex_corrections),
                    "llm_validations": len(result.llm_validations),
                    "llm_corrections": len(result.llm_corrections),
                    "hallucinations_detected": len(ctx.hallucination_detector.records),
                    "providers_used": result.providers_used,
                })
            else:
                # ê¸°ì¡´ regex ì „ìš© ValidationAgent ì‚¬ìš©
                from agents.validation_agent import get_validation_agent

                validation_agent = get_validation_agent()
                result = validation_agent.validate(
                    analyzed_data=analyzed_data,
                    original_text=ctx.parsed_data.raw_text,
                    filename=ctx.raw_input.filename
                )

                if result.success:
                    # ê²€ì¦ëœ ë°ì´í„°ë¡œ ì¶”ê°€ ì œì•ˆ
                    for field_name, value in result.validated_data.items():
                        if value != analyzed_data.get(field_name):
                            boost = result.confidence_adjustments.get(field_name, 0)
                            ctx.propose(
                                "validation_agent", field_name,
                                value,
                                0.8 + boost,
                                "ValidationAgent ê²€ì¦ ê²°ê³¼"
                            )

                    # ë³´ì • ì‚¬í•­ ë¡œê¹…
                    for correction in result.corrections:
                        logger.info(
                            f"[Orchestrator] Validation correction: {correction['field']}: "
                            f"{correction['original']} â†’ {correction['corrected']}"
                        )

                # í™˜ê° íƒì§€ (ê¸°ì¡´ ë°©ì‹)
                if self.feature_flags.use_hallucination_detection:
                    self._detect_hallucinations(ctx, analyzed_data)

                ctx.complete_stage("validation", {
                    "corrections": len(result.corrections) if result else 0,
                    "hallucinations_detected": len(ctx.hallucination_detector.records),
                })

            return {"success": True}

        except Exception as e:
            ctx.warning_collector.add(
                "VALIDATION_ERROR",
                f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}",
                severity="warning",
                user_visible=False
            )
            ctx.complete_stage("validation", {"error": str(e)})
            return {"success": True}  # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    def _detect_hallucinations(self, ctx: PipelineContext, analyzed_data: Dict[str, Any]):
        """í™˜ê° íƒì§€ ìˆ˜í–‰"""
        fields_to_check = ["exp_years", "current_company", "current_position"]

        for field_name in fields_to_check:
            value = analyzed_data.get(field_name)
            if value:
                is_valid = ctx.verify_hallucination(field_name, value, "analyst_agent")
                if not is_valid:
                    logger.warning(f"[Orchestrator] Hallucination detected: {field_name}={value}")

    async def _stage_coverage_calculation(self, ctx: PipelineContext) -> Dict[str, Any]:
        """
        Stage 6.5: ì»¤ë²„ë¦¬ì§€ ê³„ì‚° (Phase 1)

        í•„ë“œ ì™„ì„±ë„ë¥¼ ê³„ì‚°í•˜ê³  ê°­ í•„ë§ ëŒ€ìƒ í•„ë“œë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
        """
        if not self.feature_flags.use_coverage_calculator:
            logger.debug("[Orchestrator] Coverage calculation disabled")
            return {"success": True, "coverage_score": 0.0, "gap_fill_candidates": []}

        ctx.start_stage("coverage_calculation", "coverage_calculator")

        try:
            calculator = self._get_coverage_calculator()
            if not calculator:
                ctx.complete_stage("coverage_calculation", {"skipped": True})
                return {"success": True, "coverage_score": 0.0, "gap_fill_candidates": []}

            # í˜„ì¬ ê²°ì •ëœ ë°ì´í„° ìˆ˜ì§‘
            decisions = ctx.decision_manager.decide_all()
            analyzed_data = {
                name: d.final_value for name, d in decisions.items()
                if d.final_value is not None
            }

            # í•„ë“œë³„ ì‹ ë¢°ë„ ìˆ˜ì§‘
            field_confidence = {
                name: d.confidence for name, d in decisions.items()
                if d.confidence is not None
            }

            # ì¦ê±° ë§µ ìˆ˜ì§‘ (evidence_storeê°€ ìˆëŠ” ê²½ìš°)
            evidence_map = {}
            if hasattr(ctx, 'evidence_store') and ctx.evidence_store:
                for field_name in analyzed_data.keys():
                    evidence = ctx.evidence_store.get_evidence(field_name)
                    if evidence:
                        evidence_map[field_name] = evidence

            result = calculator.calculate(
                analyzed_data=analyzed_data,
                evidence_map=evidence_map,
                original_text=ctx.parsed_data.raw_text,
                field_confidence=field_confidence,
            )

            ctx.complete_stage("coverage_calculation", {
                "coverage_score": result.coverage_score,
                "evidence_backed_ratio": result.evidence_backed_ratio,
                "missing_fields_count": len(result.missing_fields),
                "low_confidence_count": len(result.low_confidence_fields),
                "gap_fill_candidates_count": len(result.gap_fill_candidates),
                "critical_coverage": result.critical_coverage,
                "important_coverage": result.important_coverage,
                "optional_coverage": result.optional_coverage,
            })

            logger.info(
                f"[Orchestrator] Coverage calculated: {result.coverage_score:.1f}%, "
                f"missing={len(result.missing_fields)}, "
                f"gap_candidates={len(result.gap_fill_candidates)}"
            )

            return {
                "success": True,
                "coverage_score": result.coverage_score,
                "gap_fill_candidates": result.gap_fill_candidates,
                "missing_fields": result.missing_fields,
                "low_confidence_fields": result.low_confidence_fields,
                "field_coverages": result.field_coverages,
            }

        except Exception as e:
            logger.warning(f"[Orchestrator] Coverage calculation error (continuing): {e}")
            ctx.complete_stage("coverage_calculation", {"error": str(e), "quality_degraded": True})
            # ì»¤ë²„ë¦¬ì§€ ê³„ì‚° ì‹¤íŒ¨ - í’ˆì§ˆ ì ìˆ˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŒ
            ctx.warning_collector.add(
                "COVERAGE_CALC_FAILED",
                f"í•„ë“œ ì™„ì„±ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)[:100]}",
                severity="warning",
                user_visible=False  # ë‚´ë¶€ ë©”íŠ¸ë¦­ì´ë¯€ë¡œ ì‚¬ìš©ìì—ê²Œ ë¯¸í‘œì‹œ
            )
            return {
                "success": True,
                "coverage_score": 0.0,
                "gap_fill_candidates": [],
                "quality_flag": "coverage_calc_failed"
            }

    async def _stage_gap_filling(
        self,
        ctx: PipelineContext,
        coverage_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Stage 6.6: ê°­ í•„ë§ (Phase 1)

        ë¹ˆ í•„ë“œì— ëŒ€í•´ íƒ€ê²Ÿ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.
        """
        if not self.feature_flags.use_gap_filler:
            logger.debug("[Orchestrator] Gap filling disabled")
            return {"success": True, "filled_count": 0}

        gap_candidates = coverage_result.get("gap_fill_candidates", [])
        if not gap_candidates:
            logger.debug("[Orchestrator] No gap candidates to fill")
            return {"success": True, "filled_count": 0}

        ctx.start_stage("gap_filling", "gap_filler_agent")

        try:
            gap_filler = self._get_gap_filler_agent()
            if not gap_filler:
                ctx.complete_stage("gap_filling", {"skipped": True})
                return {"success": True, "filled_count": 0}

            # í˜„ì¬ ê²°ì •ëœ ë°ì´í„° ìˆ˜ì§‘
            decisions = ctx.decision_manager.decide_all()
            current_data = {
                name: d.final_value for name, d in decisions.items()
                if d.final_value is not None
            }

            coverage_score = coverage_result.get("coverage_score", 0.0)

            result = await gap_filler.fill_gaps(
                gap_candidates=gap_candidates,
                current_data=current_data,
                original_text=ctx.parsed_data.raw_text,
                coverage_score=coverage_score,
            )

            # ì±„ì›Œì§„ í•„ë“œë¥¼ ì œì•ˆìœ¼ë¡œ ì¶”ê°€
            filled_count = 0
            if result.success and result.filled_fields:
                for field_name, value in result.filled_fields.items():
                    ctx.propose(
                        "gap_filler_agent", field_name,
                        value,
                        0.85,  # GapFiller ê¸°ë³¸ ì‹ ë¢°ë„
                        "GapFiller ì¬ì¶”ì¶œ"
                    )
                    filled_count += 1
                    logger.info(f"[Orchestrator] Gap filled: {field_name}")

            ctx.complete_stage("gap_filling", {
                "skipped": result.skipped,
                "filled_count": filled_count,
                "still_missing_count": len(result.still_missing),
                "total_llm_calls": result.total_llm_calls,
                "total_retries": result.total_retries,
                "processing_time_ms": result.processing_time_ms,
            })

            logger.info(
                f"[Orchestrator] Gap filling complete: filled={filled_count}, "
                f"still_missing={len(result.still_missing)}, "
                f"llm_calls={result.total_llm_calls}"
            )

            return {
                "success": True,
                "filled_count": filled_count,
                "still_missing": result.still_missing,
                "skipped": result.skipped,
            }

        except Exception as e:
            logger.warning(f"[Orchestrator] Gap filling error (continuing): {e}")
            ctx.complete_stage("gap_filling", {"error": str(e), "quality_degraded": True})
            # ê°­ í•„ë§ ì‹¤íŒ¨ - ë¹ˆ í•„ë“œê°€ ì±„ì›Œì§€ì§€ ì•ŠìŒ
            ctx.warning_collector.add(
                "GAP_FILL_FAILED",
                f"ë¹ˆ í•„ë“œ ì¬ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:100]}",
                severity="warning",
                user_visible=False  # ë‚´ë¶€ í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ ì‚¬ìš©ìì—ê²Œ ë¯¸í‘œì‹œ
            )
            return {
                "success": True,
                "filled_count": 0,
                "quality_flag": "gap_fill_failed"
            }

    def _check_quality_gate(
        self,
        coverage_result: Dict[str, Any],
        ctx: PipelineContext
    ) -> Dict[str, Any]:
        """
        ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬

        ìµœì†Œ í’ˆì§ˆ ì¡°ê±´ì„ í™•ì¸í•˜ê³  ê²½ê³ /í†µê³¼ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            coverage_result: ì»¤ë²„ë¦¬ì§€ ê³„ì‚° ê²°ê³¼
            ctx: PipelineContext

        Returns:
            Dict with:
            - passed: í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼ ì—¬ë¶€
            - warnings: í’ˆì§ˆ ê²½ê³  ëª©ë¡
            - quality_flag: í’ˆì§ˆ í”Œë˜ê·¸ (ìˆëŠ” ê²½ìš°)
        """
        if not self.feature_flags.enable_quality_gate:
            return {"passed": True, "warnings": []}

        warnings = []
        quality_flags = []

        coverage_score = coverage_result.get("coverage_score", 0.0)
        critical_coverage = coverage_result.get("critical_coverage", 0.0)

        # ì „ì²´ ì»¤ë²„ë¦¬ì§€ ì²´í¬
        if coverage_score < self.feature_flags.min_coverage_score:
            warning_msg = (
                f"í•„ë“œ ì™„ì„±ë„ê°€ ë‚®ìŠµë‹ˆë‹¤: {coverage_score:.1f}% "
                f"(ìµœì†Œ ê¶Œì¥: {self.feature_flags.min_coverage_score:.1f}%)"
            )
            warnings.append(warning_msg)
            ctx.warning_collector.add(
                "LOW_COVERAGE",
                warning_msg,
                severity="warning",
                user_visible=True
            )
            quality_flags.append("low_coverage")
            logger.warning(f"[Orchestrator] Quality gate warning: {warning_msg}")

        # Critical í•„ë“œ ì»¤ë²„ë¦¬ì§€ ì²´í¬
        if critical_coverage < self.feature_flags.min_critical_coverage:
            warning_msg = (
                f"í•µì‹¬ í•„ë“œ ì™„ì„±ë„ê°€ ë‚®ìŠµë‹ˆë‹¤: {critical_coverage:.1f}% "
                f"(ìµœì†Œ ê¶Œì¥: {self.feature_flags.min_critical_coverage:.1f}%)"
            )
            warnings.append(warning_msg)
            ctx.warning_collector.add(
                "LOW_CRITICAL_COVERAGE",
                warning_msg,
                severity="warning",
                user_visible=True
            )
            quality_flags.append("low_critical_coverage")
            logger.warning(f"[Orchestrator] Quality gate warning: {warning_msg}")

        passed = len(warnings) == 0
        result = {
            "passed": passed,
            "warnings": warnings,
        }

        if quality_flags:
            result["quality_flag"] = "quality_gate_warning"

        if not passed:
            logger.info(
                f"[Orchestrator] Quality gate: WARNINGS ({len(warnings)} issues), "
                f"coverage={coverage_score:.1f}%, critical={critical_coverage:.1f}%"
            )
        else:
            logger.info(
                f"[Orchestrator] Quality gate: PASSED, "
                f"coverage={coverage_score:.1f}%, critical={critical_coverage:.1f}%"
            )

        return result

    async def _stage_privacy(self, ctx: PipelineContext) -> Dict[str, Any]:
        """Stage 7: PII ë§ˆìŠ¤í‚¹ + ì•”í˜¸í™”"""
        ctx.start_stage("privacy", "privacy_agent")

        try:
            from agents.privacy_agent import get_privacy_agent

            # í˜„ì¬ ê²°ì •ëœ ë°ì´í„°
            decisions = ctx.decision_manager.decide_all()
            analyzed_data = {
                name: d.final_value for name, d in decisions.items()
                if d.final_value is not None
            }

            privacy_agent = get_privacy_agent()
            result = privacy_agent.process(analyzed_data)

            pii_count = 0
            if result.success:
                pii_count = len(result.pii_found)

                # ë§ˆìŠ¤í‚¹ëœ ë°ì´í„°ë¡œ CurrentData ì—…ë°ì´íŠ¸
                for field_name, value in result.masked_data.items():
                    if hasattr(ctx.current_data, field_name):
                        setattr(ctx.current_data, field_name, value)

            ctx.complete_stage("privacy", {
                "pii_count": pii_count,
                "encrypted_fields": list(result.encrypted_store.keys()) if result.success else [],
            })

            return {
                "success": True,
                "pii_count": pii_count,
                "encrypted_store": result.encrypted_store if result.success else {},
                "hash_store": {},  # hash_storeëŠ” save ë‹¨ê³„ì—ì„œ ìƒì„±
            }

        except Exception as e:
            ctx.warning_collector.add(
                "PRIVACY_ERROR",
                f"PII ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}",
                severity="warning",
                user_visible=False
            )
            ctx.complete_stage("privacy", {"error": str(e)})
            return {"success": True, "pii_count": 0}

    async def _stage_embedding(self, ctx: PipelineContext) -> Dict[str, Any]:
        """Stage 8: ì„ë² ë”© ìƒì„±"""
        ctx.start_stage("embedding", "embedding_service")

        try:
            from services.embedding_service import get_embedding_service

            # í˜„ì¬ ê²°ì •ëœ ë°ì´í„°
            decisions = ctx.decision_manager.decide_all()
            analyzed_data = {
                name: d.final_value for name, d in decisions.items()
                if d.final_value is not None
            }

            embedding_service = get_embedding_service()
            result = await embedding_service.process_candidate(
                data=analyzed_data,
                generate_embeddings=True,
                raw_text=ctx.parsed_data.raw_text
            )

            if result and result.success:
                ctx.complete_stage("embedding", {
                    "chunk_count": len(result.chunks),
                    "total_tokens": result.total_tokens,
                })
                return {
                    "success": True,
                    "chunk_count": len(result.chunks),
                    "chunks": result.chunks,
                }
            else:
                ctx.warning_collector.add(
                    "EMBEDDING_FAILED",
                    "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨",
                    severity="warning"
                )
                ctx.complete_stage("embedding", {"error": result.error if result else "Unknown"})
                return {"success": False, "chunk_count": 0, "chunks": []}

        except Exception as e:
            ctx.warning_collector.add(
                "EMBEDDING_ERROR",
                f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}",
                severity="warning",
                user_visible=False
            )
            ctx.complete_stage("embedding", {"error": str(e)})
            return {"success": False, "chunk_count": 0, "chunks": []}

    async def _stage_save(
        self,
        ctx: PipelineContext,
        user_id: str,
        job_id: str,
        mode: str,
        candidate_id: Optional[str]
    ) -> Dict[str, Any]:
        """Stage 9: DB ì €ì¥"""
        ctx.start_stage("save", "database_service")

        try:
            from services.database_service import get_database_service
            from agents.privacy_agent import get_privacy_agent

            db_service = get_database_service()

            # ëª¨ë“  ê²°ì • í™•ì •
            decisions = ctx.decide_all()
            logger.info(f"[Orchestrator] decide_all completed: {len(decisions)} decisions")

            # ìµœì¢… ë°ì´í„° ì¤€ë¹„
            analyzed_data = ctx.current_data.to_candidate_dict()

            # ë””ë²„ê·¸: ì €ì¥í•  ë°ì´í„° ë¡œê¹…
            key_fields = ["name", "exp_years", "skills", "careers", "summary", "educations"]
            for field in key_fields:
                value = analyzed_data.get(field)
                if isinstance(value, list):
                    logger.info(f"[Orchestrator] Save data - {field}: {len(value)} items")
                elif value is not None:
                    preview = str(value)[:50] if isinstance(value, str) else value
                    logger.info(f"[Orchestrator] Save data - {field}: {preview}")

            # í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)
            privacy_agent = get_privacy_agent()
            hash_store = {}
            if ctx.pii_store.phone:
                hash_store["phone"] = privacy_agent.hash_for_dedup(ctx.pii_store.phone)
            if ctx.pii_store.email:
                hash_store["email"] = privacy_agent.hash_for_dedup(ctx.pii_store.email)

            # DB ì €ì¥
            save_result = db_service.save_candidate(
                user_id=user_id,
                job_id=job_id,
                analyzed_data=analyzed_data,
                confidence_score=ctx.current_data.overall_confidence / 100,
                field_confidence={
                    k: v / 100 for k, v in ctx.current_data.confidence_scores.items()
                },
                warnings=[w.to_dict() for w in ctx.warning_collector.get_user_visible()],
                encrypted_store={},  # Privacy Agentì—ì„œ ì²˜ë¦¬ë¨
                hash_store=hash_store,
                source_file=ctx.raw_input.file_path or "",
                file_type=ctx.raw_input.file_extension,
                analysis_mode=mode,
                candidate_id=candidate_id,
            )

            if not save_result.success:
                ctx.fail_stage("save", save_result.error or "DB ì €ì¥ ì‹¤íŒ¨")
                return {"success": False, "error": save_result.error}

            # ì²­í¬ ì €ì¥
            chunks_saved = 0
            embedding_result = ctx.stage_results.results.get("embedding")
            if embedding_result and embedding_result.output.get("chunks"):
                chunks = embedding_result.output["chunks"]
                chunks_saved = db_service.save_chunks_with_embeddings(
                    candidate_id=save_result.candidate_id,
                    chunks=chunks
                )

            ctx.complete_stage("save", {
                "candidate_id": save_result.candidate_id,
                "chunks_saved": chunks_saved,
                "is_update": save_result.is_update,
            })

            return {
                "success": True,
                "candidate_id": save_result.candidate_id,
                "chunks_saved": chunks_saved,
                "is_update": save_result.is_update,
                "parent_id": save_result.parent_id,
            }

        except Exception as e:
            ctx.fail_stage("save", str(e))
            return {"success": False, "error": str(e)}

    def _create_error_result(
        self,
        ctx: PipelineContext,
        error: str,
        error_code: str,
        start_time: float
    ) -> OrchestratorResult:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        processing_time = int((time.time() - start_time) * 1000)

        return OrchestratorResult(
            success=False,
            error=error,
            error_code=error_code,
            processing_time_ms=processing_time,
            pipeline_id=ctx.metadata.pipeline_id,
            warnings=[w.message for w in ctx.warning_collector.warnings],
            context_summary=ctx.to_dict() if self.feature_flags.debug_pipeline else None,
        )

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ìƒˆë¡œìš´ ì§„ì…ì  ë©”ì„œë“œë“¤ (main.py, tasks.py í†µí•©ìš©)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    async def run_from_storage(
        self,
        storage_path: str,
        filename: str,
        user_id: str,
        job_id: str,
        mode: str = "phase_1",
        candidate_id: Optional[str] = None,
        is_retry: bool = False,
        skip_credit_deduction: bool = False,
    ) -> OrchestratorResult:
        """
        Storageì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        tasks.pyì˜ full_pipeline()ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì§„ì…ì ì…ë‹ˆë‹¤.

        Args:
            storage_path: Supabase Storage ê²½ë¡œ (ì˜ˆ: "resumes/{user_id}/{filename}")
            filename: ì›ë³¸ íŒŒì¼ëª…
            user_id: ì‚¬ìš©ì ID
            job_id: ì‘ì—… ID
            mode: ë¶„ì„ ëª¨ë“œ (phase_1 or phase_2)
            candidate_id: ê¸°ì¡´ í›„ë³´ì ID (ì—…ë°ì´íŠ¸/ì¬ì‹œë„ìš©)
            is_retry: ì¬ì‹œë„ ì—¬ë¶€
            skip_credit_deduction: í¬ë ˆë”§ ì°¨ê° ìŠ¤í‚µ ì—¬ë¶€

        Returns:
            OrchestratorResult with processing results
        """
        start_time = time.time()

        try:
            # Storageì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            file_bytes = await self._download_from_storage(storage_path)

            if not file_bytes:
                return OrchestratorResult(
                    success=False,
                    error=f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {storage_path}",
                    error_code="DOWNLOAD_FAILED",
                    processing_time_ms=int((time.time() - start_time) * 1000),
                )

            # ê¸°ì¡´ run() ë©”ì„œë“œ í˜¸ì¶œ
            result = await self.run(
                file_bytes=file_bytes,
                filename=filename,
                user_id=user_id,
                job_id=job_id,
                mode=mode,
                candidate_id=candidate_id,
                is_retry=is_retry,
            )

            # ì¶”ê°€ ì²˜ë¦¬: í¬ë ˆë”§ ì°¨ê°, ìë™ ë§¤ì¹­ ë“±
            if result.success and result.candidate_id:
                await self._post_process(
                    result=result,
                    user_id=user_id,
                    job_id=job_id,
                    skip_credit_deduction=skip_credit_deduction,
                )

            return result

        except Exception as e:
            logger.error(f"[Orchestrator] run_from_storage error: {e}", exc_info=True)
            return OrchestratorResult(
                success=False,
                error=str(e),
                error_code="INTERNAL_ERROR",
                processing_time_ms=int((time.time() - start_time) * 1000),
            )

    async def run_from_text(
        self,
        text: str,
        file_url: str,
        filename: str,
        file_type: str,
        user_id: str,
        job_id: str,
        candidate_id: str,
        mode: str = "phase_1",
        skip_credit_deduction: bool = False,
    ) -> OrchestratorResult:
        """
        ì´ë¯¸ íŒŒì‹±ëœ í…ìŠ¤íŠ¸ë¶€í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (íŒŒì‹± ìŠ¤í‚µ)

        main.pyì˜ /analyze-only ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì§„ì…ì ì…ë‹ˆë‹¤.
        íŒŒì‹± ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  AI ë¶„ì„ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.

        Args:
            text: ì´ë¯¸ íŒŒì‹±ëœ ì´ë ¥ì„œ í…ìŠ¤íŠ¸
            file_url: ì›ë³¸ íŒŒì¼ Storage ê²½ë¡œ
            filename: ì›ë³¸ íŒŒì¼ëª…
            file_type: íŒŒì¼ íƒ€ì… (hwp, pdf, docx ë“±)
            user_id: ì‚¬ìš©ì ID
            job_id: ì‘ì—… ID
            candidate_id: í›„ë³´ì ID (ì´ë¯¸ ìƒì„±ëœ ìƒíƒœ)
            mode: ë¶„ì„ ëª¨ë“œ (phase_1 or phase_2)
            skip_credit_deduction: í¬ë ˆë”§ ì°¨ê° ìŠ¤í‚µ ì—¬ë¶€

        Returns:
            OrchestratorResult with processing results
        """
        start_time = time.time()

        # PipelineContext ìƒì„±
        ctx = PipelineContext()
        ctx.metadata.candidate_id = candidate_id
        ctx.metadata.job_id = job_id
        ctx.metadata.user_id = user_id
        ctx.metadata.config["mode"] = mode
        ctx.metadata.config["entry_point"] = "run_from_text"

        logger.info(f"[Orchestrator] Starting pipeline from text: {ctx.metadata.pipeline_id}")

        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
        metrics_collector = _get_metrics_collector()
        if metrics_collector:
            metrics_collector.start_pipeline(
                pipeline_id=ctx.metadata.pipeline_id,
                job_id=job_id,
                user_id=user_id,
                pipeline_type="from_text",
                is_retry=False,
            )

        try:
            # Stage 1-2 ìŠ¤í‚µ: ì§ì ‘ íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ì„¤ì •
            ctx.set_raw_input(b"", filename, source="pre_parsed", file_path=file_url)
            ctx.set_parsed_text(text, parsing_method="external", parsing_confidence=0.9)
            ctx.metadata.config["file_type"] = file_type

            # Stage 2.5: ë¬¸ì„œ ë¶„ë¥˜ (Phase 1)
            classification_result = await self._stage_document_classification(ctx)
            if classification_result.get("should_reject"):
                return self._create_error_result(
                    ctx, classification_result["error"], "NOT_RESUME", start_time
                )

            # Stage 3: PII ì¶”ì¶œ (ì •ê·œì‹ ì „ìš©)
            await self._stage_pii_extraction(ctx)

            # Stage 4: ì‹ ì› í™•ì¸
            identity_result = await self._stage_identity_check(ctx)
            if identity_result.get("should_reject"):
                return self._create_error_result(
                    ctx, identity_result["error"], "MULTI_IDENTITY", start_time
                )

            # Stage 5: AI ë¶„ì„
            analysis_result = await self._stage_analysis(ctx, mode)
            if not analysis_result["success"]:
                return self._create_error_result(
                    ctx, analysis_result["error"], "ANALYSIS_FAILED", start_time
                )

            # Stage 5.5: ì¶”ê°€ ì²˜ë¦¬ (URL ì¶”ì¶œ, ê²½ë ¥ ê³„ì‚°, í•™ë ¥ íŒë³„)
            await self._stage_post_analysis(ctx)

            # Stage 6: ê²€ì¦ ë° í™˜ê° íƒì§€
            await self._stage_validation(ctx)

            # Stage 6.5: ì»¤ë²„ë¦¬ì§€ ê³„ì‚° (Phase 1)
            coverage_result = await self._stage_coverage_calculation(ctx)

            # Stage 6.6: ê°­ í•„ë§ (Phase 1)
            gap_fill_result = await self._stage_gap_filling(ctx, coverage_result)

            # ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬
            quality_gate_result = self._check_quality_gate(coverage_result, ctx)

            # Stage 7: PII ë§ˆìŠ¤í‚¹ + ì•”í˜¸í™”
            privacy_result = await self._stage_privacy(ctx)

            # Stage 7.5: PDF ë³€í™˜ (ì›ë³¸ì´ PDFê°€ ì•„ë‹Œ ê²½ìš°)
            pdf_storage_path = None
            if file_type.lower() not in ["pdf"]:
                pdf_storage_path = await self._stage_pdf_conversion(ctx, file_url, user_id, job_id)

            # Stage 8: ì„ë² ë”© ìƒì„±
            embedding_result = await self._stage_embedding(ctx)

            # Stage 9: DB ì €ì¥
            save_result = await self._stage_save(ctx, user_id, job_id, mode, candidate_id)
            if not save_result["success"]:
                return self._create_error_result(
                    ctx, save_result["error"], "DB_SAVE_FAILED", start_time
                )

            # PDF URL ì—…ë°ì´íŠ¸
            if pdf_storage_path and save_result.get("candidate_id"):
                from services.database_service import get_database_service
                db_service = get_database_service()
                db_service.update_candidate_pdf_url(
                    candidate_id=save_result["candidate_id"],
                    pdf_url=pdf_storage_path
                )

            # í›„ì²˜ë¦¬: í¬ë ˆë”§ ì°¨ê°, ìë™ ë§¤ì¹­
            final_result = ctx.finalize()
            processing_time = int((time.time() - start_time) * 1000)

            # í’ˆì§ˆ í”Œë˜ê·¸ ìˆ˜ì§‘ (BUG-003: fail-open ì¶”ì )
            quality_flags = {}
            if classification_result.get("quality_flag"):
                flag = classification_result["quality_flag"]
                quality_flags[flag] = True
            if identity_result.get("quality_flag"):
                flag = identity_result["quality_flag"]
                quality_flags[flag] = True
            if coverage_result.get("quality_flag"):
                quality_flags["coverage_calc_failed"] = True
            if gap_fill_result.get("quality_flag"):
                quality_flags["gap_fill_failed"] = True
            if final_result["confidence"] < 0.7:
                quality_flags["low_confidence"] = True
            # ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ê²½ê³ 
            if quality_gate_result.get("quality_flag"):
                quality_flags["quality_gate_warning"] = True

            # í’ˆì§ˆ ì €í•˜ ì‹œ ë¡œê¹…
            if quality_flags:
                logger.warning(
                    f"[Orchestrator] Quality flags detected: {list(quality_flags.keys())}"
                )

            # ğŸŸ¡ í’ˆì§ˆ ê²Œì´íŠ¸ ê²°ê³¼ ë°˜ì˜
            completed_with_warnings = not quality_gate_result.get("passed", True)
            quality_gate_passed = quality_gate_result.get("passed", True)

            result = OrchestratorResult(
                success=True,
                candidate_id=save_result["candidate_id"],
                confidence_score=final_result["confidence"],
                field_confidence=dict(ctx.current_data.confidence_scores),
                chunk_count=embedding_result.get("chunk_count", 0),
                chunks_saved=save_result.get("chunks_saved", 0),
                pii_count=privacy_result.get("pii_count", 0),
                warnings=[w["message"] for w in final_result.get("warnings", [])],
                processing_time_ms=processing_time,
                pipeline_id=ctx.metadata.pipeline_id,
                is_update=save_result.get("is_update", False),
                parent_id=save_result.get("parent_id"),
                document_kind=classification_result.get("document_kind"),
                doc_classification_confidence=classification_result.get("confidence", 0.0),
                coverage_score=coverage_result.get("coverage_score", 0.0),
                gap_fill_count=gap_fill_result.get("filled_count", 0),
                quality_flags=quality_flags,
                completed_with_warnings=completed_with_warnings,
                quality_gate_passed=quality_gate_passed,
                # ğŸŸ¡ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰
                total_input_tokens=analysis_result.get("total_input_tokens", 0),
                total_output_tokens=analysis_result.get("total_output_tokens", 0),
                context_summary=ctx.to_dict() if self.feature_flags.debug_pipeline else None,
            )

            # í›„ì²˜ë¦¬
            await self._post_process(
                result=result,
                user_id=user_id,
                job_id=job_id,
                skip_credit_deduction=skip_credit_deduction,
                is_update=save_result.get("is_update", False),
            )

            # ë©”íŠ¸ë¦­ ì™„ë£Œ ê¸°ë¡
            if metrics_collector:
                metrics_collector.complete_pipeline(
                    pipeline_id=ctx.metadata.pipeline_id,
                    success=True,
                    text_length=len(text),
                    chunk_count=embedding_result.get("chunk_count", 0),
                    pii_count=privacy_result.get("pii_count", 0),
                    confidence_score=final_result["confidence"],
                )

            logger.info(
                f"[Orchestrator] Pipeline from text completed: {ctx.metadata.pipeline_id}, "
                f"candidate={save_result['candidate_id']}, time={processing_time}ms"
            )

            return result

        except Exception as e:
            logger.error(f"[Orchestrator] run_from_text error: {e}", exc_info=True)
            if metrics_collector:
                metrics_collector.complete_pipeline(
                    pipeline_id=ctx.metadata.pipeline_id,
                    success=False,
                    error_code="INTERNAL_ERROR",
                    error_message=str(e)[:200],
                )
            return self._create_error_result(ctx, str(e), "INTERNAL_ERROR", start_time)

    async def _download_from_storage(self, storage_path: str) -> Optional[bytes]:
        """Supabase Storageì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        from services.storage_service import get_supabase_client
        import time as time_module

        max_retries = 3
        retry_delay = 1.0

        for attempt in range(max_retries + 1):
            try:
                logger.info(f"[Orchestrator] Downloading: {storage_path} (attempt {attempt + 1})")

                supabase = get_supabase_client()
                response = supabase.storage.from_("resumes").download(storage_path)

                if response and len(response) > 0:
                    logger.info(f"[Orchestrator] Downloaded {len(response)} bytes")
                    return response
                else:
                    raise ValueError("Empty response from storage")

            except Exception as e:
                logger.warning(f"[Orchestrator] Download attempt {attempt + 1} failed: {e}")

                if attempt < max_retries:
                    wait_time = retry_delay * (2 ** attempt)
                    await asyncio.sleep(wait_time)

        logger.error(f"[Orchestrator] All download attempts failed for: {storage_path}")
        return None

    async def _stage_post_analysis(self, ctx: PipelineContext) -> Dict[str, Any]:
        """
        Stage 5.5: ë¶„ì„ í›„ ì¶”ê°€ ì²˜ë¦¬

        - URL ì¶”ì¶œ (GitHub, LinkedIn, Portfolio)
        - ê²½ë ¥ ê°œì›”ìˆ˜ ê³„ì‚°
        - í•™ë ¥ ì¡¸ì—… ìƒíƒœ íŒë³„
        """
        ctx.start_stage("post_analysis", "post_processor")

        try:
            # í˜„ì¬ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            decisions = ctx.decision_manager.decide_all()
            analyzed_data = {
                name: d.final_value for name, d in decisions.items()
                if d.final_value is not None
            }

            # 1. URL ì¶”ì¶œ
            from utils.url_extractor import extract_urls_from_text
            extracted_urls = extract_urls_from_text(ctx.parsed_data.raw_text)

            # GitHub URL: í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ 
            if extracted_urls.github_url:
                ctx.propose("url_extractor", "github_url", extracted_urls.github_url, 0.95, "ì •ê·œì‹ ì¶”ì¶œ")
                logger.info(f"[Orchestrator] GitHub URL extracted: {extracted_urls.github_url}")
            elif analyzed_data.get("github_url"):
                # LLM ê²°ê³¼ê°€ github.comì´ ì•„ë‹ˆë©´ ì œê±°
                if "github.com" not in analyzed_data["github_url"].lower():
                    ctx.propose("url_extractor", "github_url", None, 0.95, "ìœ íš¨í•˜ì§€ ì•Šì€ URL ì œê±°")
                    logger.warning("[Orchestrator] Invalid github_url from LLM removed")

            # LinkedIn URL: í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ 
            if extracted_urls.linkedin_url:
                ctx.propose("url_extractor", "linkedin_url", extracted_urls.linkedin_url, 0.95, "ì •ê·œì‹ ì¶”ì¶œ")
                logger.info(f"[Orchestrator] LinkedIn URL extracted: {extracted_urls.linkedin_url}")
            elif analyzed_data.get("linkedin_url"):
                if "linkedin.com" not in analyzed_data["linkedin_url"].lower():
                    ctx.propose("url_extractor", "linkedin_url", None, 0.95, "ìœ íš¨í•˜ì§€ ì•Šì€ URL ì œê±°")
                    logger.warning("[Orchestrator] Invalid linkedin_url from LLM removed")

            # Portfolio URL: í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ , ì—†ìœ¼ë©´ LLM ê²°ê³¼ ìœ ì§€
            if extracted_urls.portfolio_url and not analyzed_data.get("portfolio_url"):
                ctx.propose("url_extractor", "portfolio_url", extracted_urls.portfolio_url, 0.85, "ì •ê·œì‹ ì¶”ì¶œ")
                logger.info(f"[Orchestrator] Portfolio URL extracted: {extracted_urls.portfolio_url}")

            # 2. ê²½ë ¥ ê°œì›”ìˆ˜ ê³„ì‚°
            from utils.career_calculator import calculate_total_experience
            careers = analyzed_data.get("careers", [])
            if careers:
                career_summary = calculate_total_experience(careers)
                ctx.propose("career_calculator", "exp_years", career_summary.years, 0.95, "ê³„ì‚°ëœ ê²½ë ¥ ì—°ìˆ˜")
                ctx.propose("career_calculator", "exp_total_months", career_summary.total_months, 0.95, "ê³„ì‚°ëœ ì´ ê°œì›”ìˆ˜")
                ctx.propose("career_calculator", "exp_display", career_summary.format_korean(), 0.95, "í‘œì‹œìš© ê²½ë ¥")
                ctx.propose("career_calculator", "has_current_job", career_summary.has_current_job, 0.95, "í˜„ì¬ ì¬ì§ ì—¬ë¶€")
                logger.info(
                    f"[Orchestrator] Career calculated: {career_summary.years}ë…„ "
                    f"{career_summary.remaining_months}ê°œì›” ({career_summary.total_months}ê°œì›”)"
                )

            # 3. í•™ë ¥ ì¡¸ì—… ìƒíƒœ íŒë³„
            from utils.education_parser import determine_graduation_status, determine_degree_level
            educations = analyzed_data.get("educations", [])
            if educations:
                updated_educations = []
                for edu in educations:
                    edu_copy = edu.copy() if isinstance(edu, dict) else edu

                    # ì¡¸ì—… ìƒíƒœ ìë™ íŒë³„
                    end_date = edu_copy.get("end_date") or edu_copy.get("end") or edu_copy.get("graduation_date")
                    explicit_status = edu_copy.get("status") or edu_copy.get("graduation_status")
                    status = determine_graduation_status(end_date_text=end_date, explicit_status=explicit_status)
                    edu_copy["graduation_status"] = status.value

                    # í•™ìœ„ ìˆ˜ì¤€ íŒë³„
                    degree_text = " ".join(filter(None, [
                        edu_copy.get("school", ""),
                        edu_copy.get("degree", ""),
                        edu_copy.get("major", "")
                    ]))
                    degree_level = determine_degree_level(degree_text)
                    edu_copy["degree_level"] = degree_level.value

                    updated_educations.append(edu_copy)

                ctx.propose("education_parser", "educations", updated_educations, 0.9, "í•™ë ¥ ì •ë³´ ë³´ê°•")
                logger.info(f"[Orchestrator] Education parsed: {len(updated_educations)} entries")

            ctx.complete_stage("post_analysis", {
                "github_url": bool(extracted_urls.github_url),
                "linkedin_url": bool(extracted_urls.linkedin_url),
                "portfolio_url": bool(extracted_urls.portfolio_url),
                "career_calculated": bool(careers),
                "education_parsed": bool(educations),
            })

            return {"success": True}

        except Exception as e:
            logger.warning(f"[Orchestrator] Post-analysis error (continuing): {e}")
            ctx.complete_stage("post_analysis", {"error": str(e)})
            return {"success": True}  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    async def _stage_pdf_conversion(
        self,
        ctx: PipelineContext,
        file_url: str,
        user_id: str,
        job_id: str
    ) -> Optional[str]:
        """
        Stage 7.5: PDF ë³€í™˜ (ì›ë³¸ì´ PDFê°€ ì•„ë‹Œ ê²½ìš°)

        Returns:
            PDF Storage ê²½ë¡œ ë˜ëŠ” None
        """
        try:
            from services.database_service import get_database_service
            from services.pdf_converter import get_pdf_converter

            file_type = ctx.metadata.config.get("file_type", "")
            if file_type.lower() in ["pdf"]:
                return None

            logger.info(f"[Orchestrator] Converting {file_type} to PDF...")

            db_service = get_database_service()

            # íŒŒì¼ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ
            file_response = db_service.client.storage.from_("resumes").download(file_url)
            if not file_response:
                logger.warning("[Orchestrator] PDF conversion: file download failed")
                return None

            pdf_converter = get_pdf_converter()
            conversion_result = pdf_converter.convert_to_pdf(file_response, ctx.raw_input.filename)

            if conversion_result.success and conversion_result.pdf_bytes:
                pdf_storage_path = db_service.upload_converted_pdf(
                    pdf_bytes=conversion_result.pdf_bytes,
                    user_id=user_id,
                    job_id=job_id,
                )
                if pdf_storage_path:
                    logger.info(f"[Orchestrator] PDF converted and uploaded: {pdf_storage_path}")
                    return pdf_storage_path

            logger.warning(f"[Orchestrator] PDF conversion failed: {conversion_result.error}")
            return None

        except Exception as e:
            logger.warning(f"[Orchestrator] PDF conversion error (continuing): {e}")
            return None

    async def _post_process(
        self,
        result: OrchestratorResult,
        user_id: str,
        job_id: str,
        skip_credit_deduction: bool = False,
        is_update: bool = False,
    ) -> None:
        """
        íŒŒì´í”„ë¼ì¸ í›„ì²˜ë¦¬

        - í¬ë ˆë”§ ì°¨ê°
        - ê¸°ì¡´ JDì™€ ìë™ ë§¤ì¹­
        - Visual Agent (í¬íŠ¸í´ë¦¬ì˜¤ ì¸ë„¤ì¼)
        """
        if not result.success or not result.candidate_id:
            return

        from services.database_service import get_database_service
        db_service = get_database_service()

        # 1. í¬ë ˆë”§ ì°¨ê°
        if skip_credit_deduction:
            logger.info(f"[Orchestrator] Skipping credit deduction")
        elif is_update:
            logger.info(f"[Orchestrator] Duplicate update, skipping credit deduction")
        else:
            logger.info(f"[Orchestrator] Deducting credit for user {user_id}...")
            credit_deducted = db_service.deduct_credit(
                user_id=user_id,
                candidate_id=result.candidate_id,
            )
            if credit_deducted:
                logger.info(f"[Orchestrator] Credit deducted successfully")
            else:
                logger.warning(f"[Orchestrator] Failed to deduct credit")

        # 2. ê¸°ì¡´ JDì™€ ìë™ ë§¤ì¹­
        if result.chunks_saved > 0:
            logger.info(f"[Orchestrator] Running auto-match with existing positions...")
            try:
                match_result = db_service.match_candidate_to_existing_positions(
                    candidate_id=result.candidate_id,
                    user_id=user_id,
                    min_score=0.3
                )
                if match_result.get("success"):
                    logger.info(
                        f"[Orchestrator] Auto-match complete: "
                        f"{match_result.get('matched_positions', 0)}/{match_result.get('total_positions', 0)} positions"
                    )
            except Exception as e:
                logger.warning(f"[Orchestrator] Auto-match error (continuing): {e}")

        # 3. Visual Agent (í¬íŠ¸í´ë¦¬ì˜¤ ì¸ë„¤ì¼) - TODO: í•„ìš” ì‹œ í™œì„±í™”
        # í˜„ì¬ëŠ” ì„±ëŠ¥ ì˜í–¥ìœ¼ë¡œ ë¹„í™œì„±í™”


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_orchestrator: Optional[PipelineOrchestrator] = None


def get_pipeline_orchestrator() -> PipelineOrchestrator:
    """PipelineOrchestrator ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _orchestrator
    if _orchestrator is None:
        _orchestrator = PipelineOrchestrator()
    return _orchestrator
