# =====================================================
# RAI Worker 환경 변수 설정 예시
# =====================================================
# 이 파일을 복사하여 .env로 사용하세요
# cp .env.example .env

# ─────────────────────────────────────────────────────
# 기본 설정
# ─────────────────────────────────────────────────────
# 환경 (development | staging | production)
ENV=staging

# 디버그 모드 (true | false)
DEBUG=false

# 로그 레벨 (DEBUG | INFO | WARNING | ERROR)
LOG_LEVEL=INFO

# ─────────────────────────────────────────────────────
# Supabase 설정
# ─────────────────────────────────────────────────────
# Supabase 프로젝트 URL
SUPABASE_URL=https://your-project.supabase.co

# Service Role Key (서버용 - 클라이언트에 노출 금지!)
SUPABASE_SERVICE_ROLE_KEY=eyJ...

# ─────────────────────────────────────────────────────
# Redis 설정 (Job Queue)
# ─────────────────────────────────────────────────────
# Redis 연결 URL
REDIS_URL=redis://localhost:6379

# ─────────────────────────────────────────────────────
# AI 모델 설정
# ─────────────────────────────────────────────────────
# 분석 모드 (phase_1: 2-Way | phase_2: 3-Way)
ANALYSIS_MODE=phase_1

# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o
OPENAI_MINI_MODEL=gpt-4o-mini

# Google Gemini
GEMINI_API_KEY=AIza...
GEMINI_MODEL=gemini-2.0-flash

# Anthropic Claude (Phase 2에서 사용)
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Embedding 모델
EMBEDDING_MODEL=text-embedding-3-small

# LLM 신뢰도 임계값 (단일 모델 결과 채택 기준)
LLM_CONFIDENCE_THRESHOLD=0.85

# ─────────────────────────────────────────────────────
# 보안 설정
# ─────────────────────────────────────────────────────
# AES-256 암호화 키 (64자 hex string)
# 생성: openssl rand -hex 32
ENCRYPTION_KEY=your-64-character-hex-encryption-key-here

# Webhook 인증 비밀키
WEBHOOK_SECRET=your-webhook-secret

# ─────────────────────────────────────────────────────
# CORS 설정
# ─────────────────────────────────────────────────────
# 허용할 도메인 (쉼표 구분)
ALLOWED_ORIGINS=https://rai-staging.vercel.app,https://rai.vercel.app

# ─────────────────────────────────────────────────────
# 파일 처리 제한
# ─────────────────────────────────────────────────────
# 최대 파일 크기 (MB)
MAX_FILE_SIZE_MB=50

# 최대 페이지 수
MAX_PAGE_COUNT=50

# 최소 유효 텍스트 길이
MIN_TEXT_LENGTH=100

# ─────────────────────────────────────────────────────
# Webhook 설정
# ─────────────────────────────────────────────────────
# Next.js API 엔드포인트 (처리 완료 알림용)
WEBHOOK_URL=https://your-app.vercel.app/api/webhook

# ─────────────────────────────────────────────────────
# 외부 API (선택)
# ─────────────────────────────────────────────────────
# 한컴 API (HWP 파싱 백업용)
HANCOM_API_KEY=

# Sentry DSN (에러 트래킹)
SENTRY_DSN=https://...@sentry.io/...

# ─────────────────────────────────────────────────────
# Feature Flags - P0 최적화
# ─────────────────────────────────────────────────────
# HWP 전담 Queue 분리 (fast/slow queue)
USE_SPLIT_QUEUES=true

# LLM 조건부 호출 (고신뢰도 결과 시 추가 LLM 스킵)
USE_CONDITIONAL_LLM=true

# LLM 병렬 호출 (GPT-4o + Gemini 동시 호출)
USE_PARALLEL_LLM=true

# ─────────────────────────────────────────────────────
# Feature Flags - 새 파이프라인 롤아웃
# ─────────────────────────────────────────────────────
# 새 PipelineContext 기반 파이프라인 사용
USE_NEW_PIPELINE=false

# ValidationAgent에 LLM 기반 검증 사용
USE_LLM_VALIDATION=false

# 에이전트 간 메시지 버스 사용
USE_AGENT_MESSAGING=false

# 환각 탐지 기능 사용
USE_HALLUCINATION_DETECTION=true

# 증거 추적 기능 사용
USE_EVIDENCE_TRACKING=true

# 새 파이프라인 롤아웃 비율 (0.0 ~ 1.0)
# 예: 0.1 = 10%의 요청만 새 파이프라인 사용
NEW_PIPELINE_ROLLOUT_PERCENTAGE=0.0

# 새 파이프라인 사용할 특정 사용자 ID (쉼표 구분)
NEW_PIPELINE_USER_IDS=

# 파이프라인 디버그 모드 (상세 로깅)
DEBUG_PIPELINE=false
